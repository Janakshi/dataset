# -*- coding: utf-8 -*-
"""2-FYP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kX9FdIzwaeC8xE7mDslTwNVOKXxTPBH-
"""

import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

"""Load the data"""

data = pd.read_csv("/content/2-DailyVolumeBooth01.csv")

"""Split the data into features and labels """

X = data[['Date']]
y = data[['DailyVolB1', 'No_of_items']]

"""Convert date column to numeric format"""

X['Date'] = pd.to_datetime(X['Date'])
X['Date'] = pd.DatetimeIndex(X['Date']).asi8

"""Scale the data using MinMaxScaler

> >Pre-processing


"""

scaler = MinMaxScaler(feature_range=(0, 1))
y = scaler.fit_transform(y)

"""Split the data into training and testing sets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""Defining the Model"""

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(10, input_shape=(1,), activation='relu'))
model.add(tf.keras.layers.Dense(10, activation='relu'))
model.add(tf.keras.layers.Dense(2, activation='linear'))

"""Compile the model"""

model.compile(optimizer='adam', loss='mean_squared_error')

"""Create dataset from the training data"""

dataset = tf.data.Dataset.from_tensor_slices((X_train['Date'], y_train))

"""Apply preprocessing function to the dataset"""

def preprocess_fn(x, y):
    x = tf.dtypes.cast(x, tf.float32)  # convert input to float
    x = (x - tf.reduce_min(x)) / (tf.reduce_max(x) - tf.reduce_min(x))  # normalize the values
    return x, y

dataset = dataset.map(preprocess_fn).shuffle(buffer_size=1000).batch(50)

"""Create the loss function"""

loss_fn = tf.keras.losses.MeanSquaredError()

"""Create the optimizer"""

optimizer = tf.keras.optimizers.Adam()

"""Train the model"""

for epoch in range(50):
    for x, y in dataset:
        with tf.GradientTape() as tape:
            # Forward pass
            logits = model(x)
            # Compute the loss
            loss_value = loss_fn(y, logits)
            # Compute gradients
            gradients = tape.gradient(loss_value, model.trainable_variables)
            # Apply gradients to the optimizer
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))

model.fit(x, y)

"""Using the model to make predictions on the test set"""

y_pred = model.predict(X_test)

"""# Evaluate the model's performance"""

if X_train.shape != X_test.shape:
    print("Error: X_train and X_test have different shapes.")
if y_train.shape != y_test.shape:
    print("Error: y_train and y_test have different shapes.")

import numpy as np
X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)
y_train = y_train.reshape(-1, 1)
y_test = y_test.reshape(-1, 1)

"""#Using R^2 Score"""

from sklearn.metrics import r2_score

y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)

print("R^2 score:", r2)

"""#Using Mean Squared Error(MSE)"""

from sklearn.metrics import mean_squared_error

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)